{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment 2\n",
    "\n",
    "__PYTHON 3__\n",
    "\n",
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "import blocks\n",
    "from blocks.bricks import MLP, Softmax, Rectifier, Linear\n",
    "from blocks.initialization import Constant, Uniform, IsotropicGaussian\n",
    "from blocks.bricks.cost import CategoricalCrossEntropy\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.graph import ComputationGraph\n",
    "from blocks.bricks.lookup import LookupTable\n",
    "from blocks.roles import WEIGHT\n",
    "\n",
    "import theano\n",
    "from theano import tensor\n",
    "\n",
    "# ------------------------------------------------------------- #\n",
    "\n",
    "words = brown.words()\n",
    "V = list(set(words))\n",
    "v = len(V)\n",
    "\n",
    "table = LookupTable(length=v, dim=1, weights_init=IsotropicGaussian(0.01), biases_init=Constant(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Wiring a network\n",
    "\n",
    "```\n",
    "INPUT-VEC ------> PROJECTION -----> OUTPUT\n",
    "\n",
    "context-vec ----> SUM ------------> word\n",
    "\n",
    "In-Layer -------> Hidden layer ---> Out-Layer\n",
    "\n",
    "-----------------------------------------------\n",
    "\n",
    "word_ind ->LU-T---------------LIN-> Softmax\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PARAM_H_SIZE = 100\n",
    "\n",
    "x = tensor.matrix('x',dtype=\"int64\")\n",
    "y = tensor.lvector('y')\n",
    "\n",
    "in_to_h = table.apply(x)\n",
    "# now average!\n",
    "x.mean(axis=1) # figure out what that really does, why axis 1?\n",
    "#\n",
    "h_to_out = Linear(name='h_to_out', input_dim=PARAM_H_SIZE, output_dim=v,\n",
    "                  weights_init=IsotropicGaussian(0.01), biases_init=Constant(0))\n",
    "y_hat = Softmax().apply(h_to_out.apply(x))\n",
    "\n",
    "cost = CategoricalCrossEntropy().apply(y.flatten(), y_hat)\n",
    "# is there something more to do with cost?\n",
    "cg = ComputationGraph(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = # TODO\n",
    "data_stream = Flatten(DataStream.default_stream(train_set,\n",
    "                                                iteration_scheme=SequentialScheme(train_set.num_examples, batch_size=256)))\n",
    "\n",
    "algorithm = GradientDescent(cost=cost, parameters=cg.parameters,\n",
    "                     step_rule=Scale(learning_rate=0.1))\n",
    "\n",
    "test_set = # TODO\n",
    "test_stream = Flatten(DataStream.default_stream(test_set, \n",
    "                                                iteration_scheme=SequentialScheme(test_set.num_examples, batch_size=1024)))\n",
    "\n",
    "monitor = DataStreamMonitoring(variables=[cost], data_stream=test_stream, prefix=\"test\")\n",
    "main_loop = MainLoop(data_stream=data_stream, algorithm=algorithm, extensions=[monitor, FinishAfter(after_n_epochs=1), Printing()])\n",
    "main_loop.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
